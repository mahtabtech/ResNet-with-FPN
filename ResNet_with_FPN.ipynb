{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQse7rF/byzl6Zib8QHlCl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahtabtech/ResNet-with-FPN/blob/main/ResNet_with_FPN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building a Feature Pyramid Network (FPN) from a Pre-trained ResNet Model**\n",
        "\n",
        "In this example, we will load a pre-trained ResNet-50 model, extract feature maps from different layers, and use them to build a Feature Pyramid Network (FPN). An FPN is a multi-scale feature extractor that combines feature maps from different layers of a neural network to improve the network's ability to detect objects at different scales."
      ],
      "metadata": {
        "id": "VSKbzy5N1Ixw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1: Load the Pre-trained ResNet Model**\n",
        "First, we load a pre-trained ResNet-50 model from the PyTorch library."
      ],
      "metadata": {
        "id": "IOgLVNgt1Pcq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0CzEmyF0yvX",
        "outputId": "6575719e-1c8b-400e-f874-1fbb494946f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 149MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load the pre-trained ResNet-50 model\n",
        "model = torchvision.models.resnet50(pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2: Extract Feature Maps from Different Layers**\n",
        "We will extract feature maps from three different layers of the ResNet model: an early stage, a middle stage, and a late stage. These layers will provide feature maps with different levels of abstraction.\n",
        "\n",
        "Early Stage: Extract features from the initial layers, including the first convolutional layer and layer1.\n",
        "\n",
        "Middle Stage: Extract features up to layer3.\n",
        "\n",
        "Late Stage: Extract features up to layer4, which contains the most abstract representations."
      ],
      "metadata": {
        "id": "sbwoFa-A1bVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract feature maps from different layers of the ResNet model\n",
        "layer1 = nn.Sequential(*list(model.children())[:5])  # Early stage feature map\n",
        "layer3 = nn.Sequential(*list(model.children())[:7])  # Middle stage feature map\n",
        "layer4 = nn.Sequential(*list(model.children())[:8])  # Late stage feature map"
      ],
      "metadata": {
        "id": "UYaySvAe1i85"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Generate an Example Input and Obtain Feature Maps**\n",
        "To check the feature maps' sizes before merging them into the FPN, we create a random input tensor (simulating an image) and pass it through the extracted layers."
      ],
      "metadata": {
        "id": "Br8zsJOr1nFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example input tensor (e.g., a 224x224 image)\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# Get feature maps from the different layers\n",
        "feat1 = layer1(x)  # Early stage feature map\n",
        "feat3 = layer3(x)  # Middle stage feature map\n",
        "feat4 = layer4(x)  # Late stage feature map\n",
        "\n",
        "# Print the shapes of the feature maps\n",
        "print(feat1.shape, feat3.shape, feat4.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHEay9Xq1qy8",
        "outputId": "4e0265a1-896c-475e-a3fa-6fb5271ab673"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 256, 56, 56]) torch.Size([1, 1024, 14, 14]) torch.Size([1, 2048, 7, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Build the Feature Pyramid Network (FPN)**\n",
        "We build an FPN that merges the extracted feature maps. The FPN uses 1x1 convolutions to reduce the number of channels to a consistent size (e.g., 256) and then applies upsampling and merging operations to create a multi-scale feature representation."
      ],
      "metadata": {
        "id": "cbig5GrV1wdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Feature Pyramid Network (FPN)\n",
        "class CorrectedFPN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CorrectedFPN, self).__init__()\n",
        "\n",
        "        # 1x1 convolutions to reduce the number of channels to 256\n",
        "        self.conv4 = nn.Conv2d(2048, 256, kernel_size=1)  # For the deepest feature map\n",
        "        self.conv3 = nn.Conv2d(1024, 256, kernel_size=1)  # For the middle feature map\n",
        "        self.conv1 = nn.Conv2d(256, 256, kernel_size=1)   # For the shallowest feature map\n",
        "\n",
        "        # 3x3 convolution for further refinement\n",
        "        self.conv_out = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, feat1, feat3, feat4):\n",
        "        # Convert feat4 to 256 channels (deepest feature map)\n",
        "        p4 = self.conv4(feat4)  # Start with the deepest feature map\n",
        "\n",
        "        # Upsample p4 and merge with the middle stage feature map (feat3)\n",
        "        p4_upsampled = F.interpolate(p4, size=(feat3.shape[2], feat3.shape[3]), mode='nearest')\n",
        "        p3 = self.conv3(feat3) + p4_upsampled\n",
        "        p3 = self.conv_out(p3)  # Refine the combined feature map\n",
        "\n",
        "        # Upsample p3 and merge with the early stage feature map (feat1)\n",
        "        p3_upsampled = F.interpolate(p3, size=(feat1.shape[2], feat1.shape[3]), mode='nearest')\n",
        "        p1 = self.conv1(feat1) + p3_upsampled\n",
        "        p1 = self.conv_out(p1)  # Refine the combined feature map\n",
        "\n",
        "        return p1, p3, p4  # Return the FPN feature maps\n",
        "\n",
        "# Instantiate the FPN model\n",
        "fpn = CorrectedFPN()\n",
        "\n",
        "# Forward pass through the FPN with the extracted feature maps\n",
        "p1, p3, p4 = fpn(feat1, feat3, feat4)\n",
        "\n",
        "# Print the shapes of the output feature maps\n",
        "print(p1.shape, p3.shape, p4.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mH6ni71P117D",
        "outputId": "06260cda-21e7-44cf-b468-81ed01d657bf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 256, 56, 56]) torch.Size([1, 256, 14, 14]) torch.Size([1, 256, 7, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanation of the FPN Architecture**\n",
        "\n",
        "**1x1 Convolutions:** These are used to standardize the number of channels in the feature maps to 256, which is necessary for merging.\n",
        "\n",
        "**Upsampling**: The deeper feature maps are upsampled using nearest-neighbor interpolation to match the spatial dimensions of the higher-resolution feature maps.\n",
        "\n",
        "**Merging:** The upsampled feature maps are merged with the higher-resolution feature maps to create a multi-scale representation.\n",
        "\n",
        "**3x3 Convolution:** A final 3x3 convolution is applied to each merged feature map to refine the output.\n",
        "\n",
        "By combining feature maps from different stages of the ResNet model, the FPN effectively captures both fine details and abstract features, improving the model's ability to handle objects at different scales."
      ],
      "metadata": {
        "id": "YfYK4SYo1-tt"
      }
    }
  ]
}